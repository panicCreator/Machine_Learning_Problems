{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing python modules\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloading and reading the datasets\n",
    "train_dataset = pd.read_csv('train_dataset.csv')\n",
    "test_dataset = pd.read_csv('test_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    fingerprint pregnancy test  android apps beaut...\n",
       "1    finally transparant silicon case  thanks uncle...\n",
       "2    love this would go talk makememories unplug re...\n",
       "3    im wired know im george made way  iphone cute ...\n",
       "4    amazing service apple even talk question unles...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data cleaning by removing stopwords, punctuation and url link\n",
    "train_dataset['tweet'] = train_dataset['tweet'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "train_dataset['tweet'] = train_dataset['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop_words))\n",
    "train_dataset['tweet'] = train_dataset['tweet'].str.replace(r'http\\S+','')\n",
    "train_dataset['tweet'] = train_dataset['tweet'].str.replace('[^\\w\\s]','')\n",
    "train_dataset['tweet'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>fingerprint pregnanc test android app beauti c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>final transpar silicon case thank uncl yay son...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>love thi would go talk makememori unplug relax...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>im wire know im georg made way iphon cute dave...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>amaz servic appl even talk question unless pay...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0  fingerprint pregnanc test android app beauti c...\n",
       "1   2      0  final transpar silicon case thank uncl yay son...\n",
       "2   3      0  love thi would go talk makememori unplug relax...\n",
       "3   4      0  im wire know im georg made way iphon cute dave...\n",
       "4   5      1  amaz servic appl even talk question unless pay..."
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#word tokenization and stemming\n",
    "tokenized_word = train_dataset['tweet'].apply(lambda x: word_tokenize(x))\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_word = tokenized_word.apply(lambda x: [stemmer.stem(each_word) for each_word in x] )\n",
    "\n",
    "for i in range(len(stemmed_word)):\n",
    "    stemmed_word[i] = ' '.join(stemmed_word[i])\n",
    "\n",
    "train_dataset['tweet'] = stemmed_word\n",
    "train_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 987)\t1\n",
      "  (0, 793)\t1\n",
      "  (0, 989)\t1\n",
      "  (0, 871)\t1\n",
      "  (0, 156)\t1\n",
      "  (0, 305)\t1\n"
     ]
    }
   ],
   "source": [
    "#feature extraction using Bag of Words\n",
    "bow_vectorizar = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
    "feature_matrix = bow_vectorizar.fit_transform(train_dataset['tweet'])\n",
    "print(feature_matrix[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating the logistic regression model\n",
    "x_train, x_val, y_train, y_val = train_test_split(feature_matrix[:7920,:], train_dataset['label'], test_size=0.3)\n",
    "\n",
    "model = LogisticRegression(verbose=1)\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.787436084733382"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model.predict_proba(x_val)\n",
    "prediction_int = prediction[:,1] >= 0.3\n",
    "prediction_int = prediction_int.astype(np.int)\n",
    "\n",
    "f1_score(y_val, prediction_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating prediction on test dataset and saving it to the file \n",
    "test_dataset['tweet'] = test_dataset['tweet'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "test_dataset['tweet'] = test_dataset['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop_words))\n",
    "test_dataset['tweet'] = test_dataset['tweet'].str.replace(r'http\\S+','')\n",
    "test_dataset['tweet'] = test_dataset['tweet'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "\n",
    "tokenized_word = test_dataset['tweet'].apply(lambda x: word_tokenize(x))\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_word = tokenized_word.apply(lambda x: [stemmer.stem(each_word) for each_word in x] )\n",
    "\n",
    "for i in range(len(stemmed_word)):\n",
    "    stemmed_word[i] = ' '.join(stemmed_word[i])\n",
    "\n",
    "test_dataset['tweet'] = stemmed_word\n",
    "test_dataset.head()\n",
    "\n",
    "bow_vectorizar = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
    "feature_matrix = bow_vectorizar.fit_transform(test_dataset['tweet'])\n",
    "print(feature_matrix[1])\n",
    "\n",
    "\n",
    "test_pred = model.predict_proba(feature_matrix)\n",
    "test_pred_int = test_pred[:,1] >= 0.3\n",
    "test_pred_int = test_pred_int.astype(np.int)\n",
    "test_dataset['label'] = test_pred_int\n",
    "submission = test_dataset[['id','label']]\n",
    "submission.to_csv('sentiment_bow.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
